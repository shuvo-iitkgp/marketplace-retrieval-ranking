from __future__ import annotations

import argparse
import json
import logging
import sys
import time
from itertools import product
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from marketplace_search.common.config import load_config
from marketplace_search.eval.metrics import (
    category_entropy_at_k,
    coverage_at_k,
    long_tail_at_k,
    mean_reciprocal_rank,
    ndcg_at_k,
)
from marketplace_search.retrieval.dual_encoder import (
    DualEncoderModel,
    _require_torch_transformers,
    encode_corpus,
)
from marketplace_search.retrieval.faiss_retrieval import FaissIndexConfig, FaissRetriever, l2_normalize
from marketplace_search.retrieval.reranker import (
    ConstrainedExploreExploitConfig,
    add_reranker_features,
    build_candidate_frame,
    build_exposure_store,
    build_product_feature_table,
    build_user_category_affinity,
    fetch_faiss_candidates_with_scores,
    rerank_with_constrained_explore_exploit,
    resolve_category_column,
)

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(name)s | %(message)s")
logger = logging.getLogger("phase6_constrained_explore_exploit")


def load_phase2_model(artifacts_dir: Path, cfg):
    torch, _, AutoTokenizer = _require_torch_transformers()
    model_cfg = cfg.dual_encoder
    model = DualEncoderModel(model_name=model_cfg.model_name, shared_weights=model_cfg.shared_weights)
    state = torch.load(artifacts_dir / "dual_encoder_state.pt", map_location="cpu")
    model.load_state_dict(state)
    tokenizer = AutoTokenizer.from_pretrained(model_cfg.model_name)
    return model, tokenizer


def encode_queries(model, tokenizer, texts: list[str], cfg, device: str) -> np.ndarray:
    return l2_normalize(
        encode_corpus(
            model=model,
            tokenizer=tokenizer,
            texts=texts,
            device=device,
            batch_size=cfg.phase6.batch_size,
            max_length=cfg.dual_encoder.max_length,
        )
    )


def assign_query_cluster_id(text: str) -> str:
    toks = str(text).strip().lower().split()
    if not toks:
        return "unknown"
    return " ".join(toks[:2])


def compute_cluster_entropy(train_interactions: pd.DataFrame, queries_df: pd.DataFrame, catalog_df: pd.DataFrame) -> dict[str, float]:
    category_col = resolve_category_column(catalog_df)
    positives = train_interactions[train_interactions["label"] == 1][["product_id"]].copy()
    with_query = positives.merge(queries_df[["product_id", "query_text"]].drop_duplicates("product_id"), on="product_id", how="left")
    with_query = with_query.merge(
        catalog_df[["product_id", category_col]].drop_duplicates("product_id").rename(columns={category_col: "category"}),
        on="product_id",
        how="left",
    ).fillna({"category": "unknown", "query_text": ""})
    with_query["query_cluster_id"] = with_query["query_text"].map(assign_query_cluster_id)

    entropy_map: dict[str, float] = {}
    for cluster_id, frame in with_query.groupby("query_cluster_id"):
        probs = frame["category"].value_counts(normalize=True).to_numpy(dtype=np.float32)
        entropy = float(-(probs * np.log2(np.clip(probs, 1e-12, 1.0))).sum())
        entropy_map[str(cluster_id)] = entropy
    return entropy_map


def build_eval_queries(interactions_df: pd.DataFrame, queries_df: pd.DataFrame, catalog_df: pd.DataFrame, entropy_map: dict[str, float]) -> pd.DataFrame:
    positives = interactions_df[interactions_df["label"] == 1][["user_id", "product_id"]].copy()
    query_lookup = queries_df[["product_id", "query_text"]].drop_duplicates("product_id")
    category_col = resolve_category_column(catalog_df)
    category_lookup = catalog_df[["product_id", category_col]].drop_duplicates("product_id").rename(columns={category_col: "category"})
    out = (
        positives.merge(query_lookup, on="product_id", how="inner")
        .merge(category_lookup, on="product_id", how="left")
        .fillna({"category": "unknown", "query_text": ""})
        .reset_index(drop=True)
    )
    out["query_cluster_id"] = out["query_text"].map(assign_query_cluster_id)
    out["query_cluster_entropy"] = out["query_cluster_id"].map(lambda c: float(entropy_map.get(c, 0.0)))
    return out


def evaluate_topk(topk_df: pd.DataFrame, eval_k: int, catalog_df: pd.DataFrame, popularity_map: dict[str, float], bottom_threshold: float) -> dict[str, float]:
    relevant = [[pid] for pid in topk_df.groupby("query_idx")["target_product_id"].first().tolist()]
    rankings = topk_df.sort_values(["query_idx", "final_rank"]).groupby("query_idx")["candidate_product_id"].apply(list).tolist()
    categories = topk_df.sort_values(["query_idx", "final_rank"]).groupby("query_idx")["candidate_category"].apply(list).tolist()

    return {
        f"ndcg@{eval_k}": float(ndcg_at_k(relevant, rankings, k=eval_k)),
        "mrr": float(mean_reciprocal_rank(relevant, rankings, k=eval_k)),
        f"coverage@{eval_k}": float(coverage_at_k(rankings, k=eval_k, catalog_size=catalog_df["product_id"].nunique())),
        f"longtail@{eval_k}": float(long_tail_at_k(rankings, popularity_map, bottom_quantile_threshold=bottom_threshold, k=eval_k)),
        f"category_entropy@{eval_k}": float(category_entropy_at_k(categories, k=eval_k)),
    }


def main(config_path: str) -> None:
    cfg = load_config(config_path, project_root=PROJECT_ROOT)
    processed_dir = PROJECT_ROOT / cfg.paths.processed_dir
    artifacts_dir = PROJECT_ROOT / cfg.paths.artifacts_dir
    logs_dir = PROJECT_ROOT / cfg.paths.logs_dir
    logs_dir.mkdir(parents=True, exist_ok=True)

    phase2_best = artifacts_dir / "dual_encoder_best"
    if not phase2_best.exists():
        raise FileNotFoundError("Phase 2 artifacts not found. Run scripts/05_train_eval_dual_encoder.py first.")

    train_df = pd.read_csv(processed_dir / "train.csv.gz")
    val_df = pd.read_csv(processed_dir / "val.csv.gz")
    test_df = pd.read_csv(processed_dir / "test.csv.gz")
    catalog_df = pd.read_csv(processed_dir / "rich_catalog.csv.gz")
    queries_df = pd.read_csv(processed_dir / "queries_title.csv.gz")

    model, tokenizer = load_phase2_model(phase2_best, cfg)
    torch, _, _ = _require_torch_transformers()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    catalog_ids = catalog_df["product_id"].astype(str).tolist()
    catalog_texts = catalog_df["cleaned_text"].fillna("").tolist()

    embeddings_path = artifacts_dir / cfg.phase6.catalog_embeddings_filename
    if embeddings_path.exists():
        catalog_embeddings = np.load(embeddings_path).astype(np.float32)
    else:
        catalog_embeddings = l2_normalize(
            encode_corpus(
                model=model,
                tokenizer=tokenizer,
                texts=catalog_texts,
                device=device,
                batch_size=cfg.phase6.batch_size,
                max_length=cfg.dual_encoder.max_length,
            )
        ).astype(np.float32)
        np.save(embeddings_path, catalog_embeddings)

    retriever = FaissRetriever(product_ids=catalog_ids, config=FaissIndexConfig(index_type="flatip", metric="ip"))
    retriever.fit(catalog_embeddings)

    product_feature_table = build_product_feature_table(train_df, catalog_df, ctr_smoothing=float(cfg.phase5.ctr_smoothing))
    user_cat_affinity = build_user_category_affinity(train_df, catalog_df, smoothing=float(cfg.phase5.user_affinity_smoothing))

    entropy_map = compute_cluster_entropy(train_df, queries_df, catalog_df)
    split_queries = {
        "train": build_eval_queries(train_df, queries_df, catalog_df, entropy_map),
        "val": build_eval_queries(val_df, queries_df, catalog_df, entropy_map),
        "test": build_eval_queries(test_df, queries_df, catalog_df, entropy_map),
    }

    split_candidates: dict[str, pd.DataFrame] = {}
    for split_name, eval_df in split_queries.items():
        query_embeddings = encode_queries(model, tokenizer, eval_df["query_text"].fillna("").tolist(), cfg, device)
        cand_ids, cand_scores = fetch_faiss_candidates_with_scores(retriever, query_embeddings, k=int(cfg.phase6.candidate_k))
        candidate_df = build_candidate_frame(eval_df, cand_ids, cand_scores)
        candidate_df = candidate_df.merge(
            eval_df[["user_id", "product_id", "query_text", "query_cluster_id", "query_cluster_entropy"]],
            left_on=["user_id", "target_product_id"],
            right_on=["user_id", "product_id"],
            how="left",
        ).drop(columns=["product_id"])
        candidate_df, feature_cols = add_reranker_features(candidate_df, product_feature_table, user_cat_affinity)
        split_candidates[split_name] = candidate_df

    x_train = split_candidates["train"][feature_cols].to_numpy(dtype=np.float32)
    y_train = split_candidates["train"]["label"].to_numpy(dtype=np.int32)
    model_pipe = Pipeline(
        steps=[
            ("scaler", StandardScaler()),
            (
                "mlp",
                MLPClassifier(
                    hidden_layer_sizes=(int(cfg.phase5.hidden_dim),),
                    activation="relu",
                    alpha=float(cfg.phase5.weight_decay),
                    learning_rate_init=float(cfg.phase5.learning_rate),
                    max_iter=int(cfg.phase5.max_iter),
                    early_stopping=True,
                    random_state=int(cfg.project.seed),
                ),
            ),
        ]
    )
    model_pipe.fit(x_train, y_train)

    for split_name, frame in split_candidates.items():
        probs = model_pipe.predict_proba(frame[feature_cols].to_numpy(dtype=np.float32))[:, 1]
        frame["rerank_probability"] = probs
        sim = frame["similarity_score"].to_numpy(dtype=np.float32)
        sim_norm = (sim - float(sim.min())) / max(float(sim.max() - sim.min()), 1e-12)
        frame["normalized_similarity"] = sim_norm
        frame["base_score"] = frame["rerank_probability"] if bool(cfg.phase6.use_reranker_prob) else frame["normalized_similarity"]

    popularity = train_df.groupby("product_id").size().astype(float)
    popularity_map = {str(pid): float(cnt) for pid, cnt in popularity.items()}
    bottom_q = float(np.quantile(np.array(list(popularity_map.values()), dtype=np.float32), float(cfg.phase6.long_tail_quantile)))

    sweep_results = []
    for epsilon, alpha, top_pool_k, max_per_category in product(
        cfg.phase6.epsilon_grid,
        cfg.phase6.alpha_grid,
        cfg.phase6.top_pool_k_grid,
        cfg.phase6.max_per_category_grid,
    ):
        ee_cfg = ConstrainedExploreExploitConfig(
            epsilon=float(epsilon),
            alpha=float(alpha),
            top_pool_k=int(top_pool_k),
            eval_k=int(cfg.phase6.eval_k),
            max_per_category=int(max_per_category),
            broad_entropy_threshold=float(cfg.phase6.entropy_threshold),
            exposure_decay=float(cfg.phase6.exposure_decay),
        )

        exposure_store = build_exposure_store()
        _, exposure_store = rerank_with_constrained_explore_exploit(split_candidates["train"], exposure_store, ee_cfg, seed=int(cfg.project.seed))
        val_topk, exposure_store = rerank_with_constrained_explore_exploit(split_candidates["val"], exposure_store, ee_cfg, seed=int(cfg.project.seed))
        test_topk, exposure_store = rerank_with_constrained_explore_exploit(split_candidates["test"], exposure_store, ee_cfg, seed=int(cfg.project.seed))

        val_metrics = evaluate_topk(val_topk, int(cfg.phase6.eval_k), catalog_df, popularity_map, bottom_q)
        test_metrics = evaluate_topk(test_topk, int(cfg.phase6.eval_k), catalog_df, popularity_map, bottom_q)

        baseline_df = split_candidates["test"].sort_values(["query_idx", "base_score"], ascending=[True, False])
        baseline_topk = baseline_df.groupby("query_idx", as_index=False).head(int(cfg.phase6.eval_k)).copy()
        baseline_topk["final_rank"] = baseline_topk.groupby("query_idx").cumcount() + 1
        baseline_metrics = evaluate_topk(baseline_topk, int(cfg.phase6.eval_k), catalog_df, popularity_map, bottom_q)

        result = {
            "epsilon": float(epsilon),
            "alpha": float(alpha),
            "top_pool_k": int(top_pool_k),
            "max_per_category": int(max_per_category),
            "val": val_metrics,
            "test": {
                **test_metrics,
                "delta_ndcg": float(test_metrics[f"ndcg@{cfg.phase6.eval_k}"] - baseline_metrics[f"ndcg@{cfg.phase6.eval_k}"]),
                "delta_mrr": float(test_metrics["mrr"] - baseline_metrics["mrr"]),
                "baseline_ndcg": float(baseline_metrics[f"ndcg@{cfg.phase6.eval_k}"]),
                "baseline_mrr": float(baseline_metrics["mrr"]),
            },
        }
        sweep_results.append(result)

    best = sorted(sweep_results, key=lambda r: (r["test"][f"coverage@{cfg.phase6.eval_k}"], r["test"][f"ndcg@{cfg.phase6.eval_k}"]), reverse=True)[0]

    report = {
        "timestamp_utc": int(time.time()),
        "phase": "phase06_constrained_explore_exploit",
        "params": {
            "eval_k": int(cfg.phase6.eval_k),
            "entropy_threshold": float(cfg.phase6.entropy_threshold),
            "exposure_decay": float(cfg.phase6.exposure_decay),
        },
        "tradeoff_curve": [
            {
                "epsilon": r["epsilon"],
                "alpha": r["alpha"],
                "top_pool_k": r["top_pool_k"],
                "coverage": r["test"][f"coverage@{cfg.phase6.eval_k}"],
                "delta_ndcg": r["test"]["delta_ndcg"],
            }
            for r in sweep_results
        ],
        "sweep_results": sweep_results,
        "best_by_coverage_then_ndcg": best,
    }

    out = logs_dir / "phase6_constrained_explore_exploit_report.json"
    with out.open("w") as f:
        json.dump(report, f, indent=2)
    logger.info("saved report -> %s", out)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="configs/default.yaml")
    args = parser.parse_args()
    main(args.config)